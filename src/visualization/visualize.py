print(news.head())
print(news.info())

plt.figure(figsize=(10, 6))
plt.scatter(range(len(news['shares'])), news['shares'], color='skyblue', alpha=0.7)
plt.title("Scatter Plot for 'shares'", fontsize=16)
plt.xlabel("Index", fontsize=14)
plt.ylabel("Shares", fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(range(len(news['shares'])), news['shares'], color='skyblue', alpha=0.7)
plt.title("Scatter Plot for 'shares'", fontsize=16)
plt.xlabel("Index", fontsize=14)
plt.ylabel("Shares", fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

print(news.columns)

print(news.columns[news.isna().any()])

print(non_numeric_features)

print("Training set size:", X_train.shape)
print("Validation set size:", X_val.shape)
print("Test set size:", X_test.shape)

plt.plot(k_v, knn_acc, marker='o')
plt.xlabel("k")
plt.ylabel("precision")
plt.title("best precision")
plt.grid()
plt.show()

plt.plot(k_v, knn_acc, marker='o')
plt.xlabel("k")
plt.ylabel("recall")
plt.title("best recall_score")
plt.grid()
plt.show()

plt.plot(k_v, knn_acc, marker='o')
plt.xlabel("k")
plt.ylabel("f1-score")
plt.title("best f1-score")
plt.grid()
plt.show()

plt.plot(max_depth_values, val_accuracies, marker='o')
plt.title("best recall-score")
plt.xlabel("max_depth")
plt.ylabel("recall-score")
plt.grid()
plt.show()

plt.plot(max_depth_values, val_accuracies, marker='o')
plt.title("best f1-score")
plt.xlabel("max_depth")
plt.ylabel("f1-score")
plt.grid()
plt.show()

plt.plot(max_depth_values, val_accuracies, marker='o')
plt.title("best precision")
plt.xlabel("max_depth")
plt.ylabel("precision")
plt.grid()
plt.show()

print("k-NN accuracy_score: ", knn_val_accuracy)
print("k-NN f1_score: ", knn_f1)
print("k-NN precision: ", knn_precision)
print("k-NN recall: ", knn_recall)
print("k-NN AUC score (Validation): ", knn_val_auc)
print("k-NN False Positive Rate (Validation): ", knn_fpr)
print("k-NN Specificity (Validation): ", knn_specificity)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix for K-NN")
plt.show()

print("Decision Tree accuracy_score: ", dt_val_accuracy)
print("Decision Tree f1_score: ", dt_f1)
print("Decision Tree precision: ", dt_precision)
print("Decision Tree recall: ", dt_recall)
print("Decision Tree AUC score: ", dt_val_auc)
print("Decision Tree False Positive Rate: ", dt_fpr)
print("Decision Tree Specificity: ", dt_specificity)

labels = ['0', '1'] 

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix for Decision Tree")
plt.show()

print("Naive Bayes accuracy_score: ", nb_val_accuracy)
print("Naive Bayes f1_score: ", nb_f1)
print("Naive Bayes precision: ", nb_precision)
print("Naive Bayes recall: ", nb_recall)
print("Naive Bayes AUC score: ", nb_val_auc)
print("Naive Bayes False Positive Rate: ", nb_fpr)
print("Naive Bayes Specificity: ", nb_specificity)

labels = ['0', '1'] 

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix for GaussianNB")
plt.show()

plt.figure(figsize=(10, 6))
plt.barh(top_10_features['Feature'], top_10_features['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Importance in Decision Tree')
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(dt_fpr, dt_tpr,label=f'Decision Tree (AUC = {dt_auc:.2f})')
plt.plot(nb_fpr, nb_tpr,label=f'Naive Bayes (AUC = {nb_auc:.2f})')
plt.plot(knn_fpr, knn_tpr,label=f'k-NN (AUC = {knn_auc:.2f})')
plt.plot([0, 1], [0, 1],label='Random Guessing')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend(loc='lower right')
plt.grid()
plt.show()

plt.figure(figsize=(18, 8))
plt.bar(x - width*3, accuracy, width, label='Accuracy')
plt.bar(x - width*2, f1, width, label='F1 Score')
plt.bar(x - width, precision, width, label='Precision')
plt.bar(x, recall, width, label='Recall')
plt.bar(x + width, auc_score, width, label='AUC Score')
plt.bar(x + width*2, specificity, width, label='Specificity')

plt.xticks(x, models)
plt.ylabel("Score")
plt.title("Model Metrics Comparison")
plt.legend()
plt.grid()
plt.show()

plt.figure(figsize=(18, 8))
plt.bar(x - width*3, accuracy, width, label='Accuracy')
plt.bar(x - width*2, f1, width, label='F1 Score')
plt.bar(x - width, precision, width, label='Precision')
plt.bar(x, recall, width, label='Recall')
plt.bar(x + width, auc_score, width, label='AUC Score')

plt.xticks(x, models)
plt.ylabel("Score")
plt.title("Model Metrics Comparison(specifiy removed")
plt.legend()
plt.grid()
plt.show()